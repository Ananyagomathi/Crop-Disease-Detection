{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import rasterio\n",
    "\n",
    "def load_vegetation_indices(file_path):\n",
    "    with rasterio.open(file_path) as src:\n",
    "        bands = src.read()  \n",
    "    vegetation_indices = torch.tensor(bands, dtype=torch.float32)  \n",
    "    return vegetation_indices\n",
    "\n",
    "def load_vegetation_indices_from_year_folder(year_folder_path):\n",
    "    indices = []\n",
    "    for month_folder in sorted(os.listdir(year_folder_path)):\n",
    "        month_path = os.path.join(year_folder_path, month_folder)\n",
    "        if not os.path.isdir(month_path):\n",
    "            continue  \n",
    "\n",
    "        for file_name in sorted(os.listdir(month_path)):\n",
    "            file_path = os.path.join(month_path, file_name)\n",
    "            if file_path.endswith(\".png\"):\n",
    "                with Image.open(file_path) as img:\n",
    "                    img_array = np.array(img.convert(\"L\"))  \n",
    "                    indices.append(torch.tensor(img_array, dtype=torch.float32))\n",
    "    \n",
    "    if not indices:\n",
    "        raise ValueError(f\"No valid vegetation index files found in {year_folder_path}\")\n",
    "    \n",
    "    vegetation_indices = torch.stack(indices, dim=0)\n",
    "    return vegetation_indices\n",
    "\n",
    "def load_ground_truth_png(file_path):\n",
    "    with Image.open(file_path) as img:\n",
    "        ground_truth = np.array(img.convert(\"L\"))  \n",
    "    return torch.tensor(ground_truth, dtype=torch.long).view(-1) \n",
    "\n",
    "def match_tensor_shapes(tensor1, tensor2):\n",
    "  \n",
    "    size1 = tensor1.size(1)  \n",
    "    size2 = tensor2.size(1)  \n",
    "\n",
    "    if size1 < size2:\n",
    "        tensor2 = tensor2[:, :size1]\n",
    "    elif size2 < size1:\n",
    "        tensor1 = tensor1[:, :size2]\n",
    "\n",
    "    return tensor1, tensor2\n",
    "\n",
    "years = [\"Year_2023\", \"Year_2022\", \"Year_2021\", \"Year_2020\", \"Year_2019\"]\n",
    "vegetation_indices_folders = {\n",
    "    \"Year_2023\": \"C:/Users/ADMIN/Downloads/year 2023_indices/year 2023-2024\",  \n",
    "    \"Year_2022\": \"C:/Users/ADMIN/Downloads/year_2022_Indices/Year_2022\",\n",
    "    \"Year_2021\": \"C:/Users/ADMIN/Downloads/year_2021_Indices/Year_2021\",\n",
    "    \"Year_2020\": \"C:/Users/ADMIN/Downloads/year_2020_Indices/Year_2020\",\n",
    "    \"Year_2019\": \"C:/Users/ADMIN/Downloads/year_2019_Indices/Year_2019\"\n",
    "}\n",
    "ground_truth_base_folder = \"C:/Users/ADMIN/Downloads/GT\" \n",
    "\n",
    "all_vegetation_data = []\n",
    "\n",
    "for year in years:\n",
    "    folder_path = vegetation_indices_folders.get(year, None)\n",
    "    if folder_path is None:\n",
    "        print(f\"No folder specified for {year}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Error: The folder {folder_path} does not exist.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        vegetation_data = load_vegetation_indices_from_year_folder(folder_path)\n",
    "        num_indices, height, width = vegetation_data.shape\n",
    "        vegetation_data = vegetation_data.view(num_indices, -1).transpose(0, 1)  \n",
    "        all_vegetation_data.append(vegetation_data)\n",
    "        print(f\"{year} Vegetation data shape: {vegetation_data.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {year} vegetation indices: {e}\")\n",
    "\n",
    "if len(all_vegetation_data) > 0:\n",
    "    min_shape = min(data.shape[1] for data in all_vegetation_data)\n",
    "    all_vegetation_data = [data[:, :min_shape] for data in all_vegetation_data]  \n",
    "    vegetation_data = torch.cat(all_vegetation_data, dim=0)\n",
    "    print(\"Combined vegetation data shape:\", vegetation_data.shape)\n",
    "else:\n",
    "    raise ValueError(\"Failed to load any vegetation indices.\")\n",
    "\n",
    "all_labels = []\n",
    "all_ground_truth_data = []\n",
    "\n",
    "for year in years:\n",
    "    year_path = os.path.join(ground_truth_base_folder, year + \"_GT\")\n",
    "    \n",
    "    if not os.path.exists(year_path):\n",
    "        print(f\"Year folder {year}_GT does not exist. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    for month_folder in os.listdir(year_path):\n",
    "        month_path = os.path.join(year_path, month_folder)\n",
    "        \n",
    "        if os.path.isdir(month_path):\n",
    "            ground_truth_files = [f for f in os.listdir(month_path) if f.endswith('.png')]\n",
    "            print(f\"Processing {len(ground_truth_files)} ground truth files in {year}/{month_folder}\")\n",
    "            \n",
    "            for ground_truth_file in ground_truth_files:\n",
    "                ground_truth_path = os.path.join(month_path, ground_truth_file)\n",
    "\n",
    "                if \"Non_Vegetation\" in ground_truth_file:\n",
    "                    label = 0  \n",
    "                elif \"Vegetation\" in ground_truth_file:\n",
    "                    label = 1  \n",
    "                else:\n",
    "                    print(f\"Skipping file {ground_truth_file} (unknown label)\")\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    ground_truth_labels = load_ground_truth_png(ground_truth_path)\n",
    "                    all_ground_truth_data.append(ground_truth_labels)\n",
    "                    all_labels.append(torch.full_like(ground_truth_labels, label))  \n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading ground truth file {ground_truth_file} in {year}/{month_folder}: {e}\")\n",
    "\n",
    "if len(all_labels) > 0 and len(all_ground_truth_data) > 0:\n",
    "    all_ground_truth_data = torch.cat(all_ground_truth_data, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    print(\"Ground truth data and labels successfully loaded and concatenated.\")\n",
    "    print(\"Ground truth data shape:\", all_ground_truth_data.shape)\n",
    "    print(\"Ground truth labels shape:\", all_labels.shape)\n",
    "else:\n",
    "    raise ValueError(\"Error: No ground truth data loaded. Please check file paths and ensure files are present.\")\n",
    "\n",
    "num_vegetation_samples = vegetation_data.size(0)\n",
    "num_label_samples = all_labels.size(0)\n",
    "\n",
    "if num_vegetation_samples > num_label_samples:\n",
    "    vegetation_data = vegetation_data[:num_label_samples]\n",
    "    print(\"Truncated vegetation data to match labels.\")\n",
    "elif num_label_samples > num_vegetation_samples:\n",
    "    all_labels = all_labels[:num_vegetation_samples]\n",
    "    all_ground_truth_data = all_ground_truth_data[:num_vegetation_samples]\n",
    "    print(\"Truncated labels to match vegetation data.\")\n",
    "\n",
    "print(\"Adjusted vegetation data shape:\", vegetation_data.shape)\n",
    "print(\"Adjusted ground truth labels shape:\", all_labels.shape)\n",
    "\n",
    "\n",
    "class VegetationDataset(Dataset):\n",
    "    def __init__(self, vegetation_data, labels):\n",
    "        self.vegetation_data = vegetation_data\n",
    "        self.labels = labels\n",
    "        assert len(self.vegetation_data) == len(self.labels), \"Data dimensions do not match\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vegetation_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.labels[idx], self.vegetation_data[idx]\n",
    "\n",
    "dataset = VegetationDataset(vegetation_data, all_labels)\n",
    "train_loader = DataLoader(dataset, batch_size=64, shuffle=True) \n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask=None):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "class LSTMAttentionModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, embed_size, heads):\n",
    "        super(LSTMAttentionModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.attention = MultiHeadSelfAttention(embed_size, heads)\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, 64)  \n",
    "        \n",
    "        self.fc2 = nn.Linear(64, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.unsqueeze(1) \n",
    "        lstm_out, _ = self.lstm(x)  \n",
    "        lstm_out = lstm_out.squeeze(1) \n",
    "\n",
    "        attention_out = self.attention(lstm_out.unsqueeze(1), lstm_out.unsqueeze(1), lstm_out.unsqueeze(1))\n",
    "        attention_out = attention_out.squeeze(1)  \n",
    "\n",
    "        x = F.relu(self.fc1(lstm_out)) \n",
    "        x = self.fc2(x)  \n",
    "        return x\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "input_size = 45  \n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "output_size = 2  \n",
    "embed_size = 256\n",
    "heads = 8\n",
    "\n",
    "model = LSTMAttentionModel(\n",
    "    input_size=input_size,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers,\n",
    "    output_size=output_size,\n",
    "    embed_size=embed_size,\n",
    "    heads=heads\n",
    ").to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (labels, vegetation_data) in enumerate(train_loader):\n",
    "        labels, vegetation_data = labels.to(device), vegetation_data.to(device)\n",
    "\n",
    "        outputs = model(vegetation_data)\n",
    "        loss = loss_fn(outputs, labels.long())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import functional as TF\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "years = [\"Year_2023\", \"Year_2022\", \"Year_2021\", \"Year_2020\", \"Year_2019\"]\n",
    "vegetation_indices_folders = {\n",
    "    \"Year_2023\": \"C:/Users/ADMIN/Downloads/year 2023_indices/year 2023-2024\", \n",
    "    \"Year_2022\": \"C:/Users/ADMIN/Downloads/year_2022_Indices/Year_2022\",\n",
    "    \"Year_2021\": \"C:/Users/ADMIN/Downloads/year_2021_Indices/Year_2021\",\n",
    "    \"Year_2020\": \"C:/Users/ADMIN/Downloads/year_2020_Indices/Year_2020\",\n",
    "    \"Year_2019\": \"C:/Users/ADMIN/Downloads/year_2019_Indices/Year_2019\"\n",
    "}\n",
    "\n",
    "class VegetationDataset(Dataset):\n",
    "    def __init__(self, folder_paths, resize=(128, 128)): \n",
    "        self.folder_paths = folder_paths  \n",
    "        self.files = []\n",
    "        self.resize = resize\n",
    "        for folder in folder_paths:\n",
    "            for root, _, files in os.walk(folder):\n",
    "                for file in files:\n",
    "                    if file.endswith(\".png\"):\n",
    "                        self.files.append(os.path.join(root, file))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.files[idx]\n",
    "        with Image.open(file_path) as img:\n",
    "            img_resized = TF.resize(img, self.resize)  \n",
    "            img_array = np.array(img_resized, dtype=np.float32) \n",
    "        return torch.tensor(img_array).permute(2, 0, 1)  \n",
    "\n",
    "folder_paths = [vegetation_indices_folders[year] for year in years]\n",
    "\n",
    "dataset = VegetationDataset(folder_paths)\n",
    "train_loader = DataLoader(dataset, batch_size=4, shuffle=True)  \n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert self.head_dim * heads == embed_size, \"Embedding size must be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask=None):\n",
    "        N = query.shape[0]  \n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys]) \n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "class RCNNWithAttention(nn.Module):\n",
    "    def __init__(self, input_channels, conv_filters, lstm_hidden_size, lstm_layers, embed_size, heads, latent_dim):\n",
    "        super(RCNNWithAttention, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(input_channels, conv_filters, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(conv_filters, conv_filters * 2, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.lstm = nn.LSTM(conv_filters * 2, lstm_hidden_size, lstm_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.attention = MultiHeadSelfAttention(embed_size, heads)\n",
    "\n",
    "        self.fc_latent = nn.Linear(embed_size, latent_dim)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_channels, height, width = x.size()\n",
    "\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)  \n",
    "        x = x.flatten(start_dim=2)  \n",
    "        x = x.permute(0, 2, 1)  \n",
    "\n",
    "        lstm_out, _ = self.lstm(x)  \n",
    "\n",
    "        attention_out = self.attention(lstm_out, lstm_out, lstm_out)  \n",
    "\n",
    "        latent_rep = self.fc_latent(attention_out.mean(dim=1))  \n",
    "        return latent_rep\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_channels = 4  \n",
    "conv_filters = 64\n",
    "lstm_hidden_size = 128\n",
    "lstm_layers = 2\n",
    "embed_size = 256\n",
    "heads = 8\n",
    "latent_dim = 64\n",
    "\n",
    "model = RCNNWithAttention(input_channels, conv_filters, lstm_hidden_size, lstm_layers, embed_size, heads, latent_dim).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)  \n",
    "        latent_rep = model(batch) \n",
    "        loss = latent_rep.norm(2).mean()  \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "model.eval()\n",
    "latent_reps = []\n",
    "with torch.no_grad():\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        latent_rep = model(batch)\n",
    "        latent_reps.append(latent_rep.cpu())\n",
    "latent_reps = torch.cat(latent_reps, dim=0)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(latent_reps.numpy())\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "reduced_latent_reps = pca.fit_transform(latent_reps.numpy())\n",
    "plt.scatter(reduced_latent_reps[:, 0], reduced_latent_reps[:, 1], c=clusters, cmap='viridis')\n",
    "plt.title('Clustering of Vegetation Indices')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
